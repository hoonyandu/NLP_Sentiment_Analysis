{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NSMC_KoELECTRA.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b15d6033799c466cb747e759b8474fab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_891594c5ce464b84ac6b02819cad096b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2ec4590d915142af9acd08cfd2de81dd","IPY_MODEL_f99f572b85cf405987aff838afcb2be7"]}},"891594c5ce464b84ac6b02819cad096b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ec4590d915142af9acd08cfd2de81dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_02480d244ad54d3aa6c62fe196d4390c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":487,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":487,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e6ab287a2d964655a42b416c721e3e4f"}},"f99f572b85cf405987aff838afcb2be7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_89cf87c5673a4edebb8d2777f7d43c04","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 487/487 [00:55&lt;00:00, 8.84B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5db3ddda9c534222b41c2b98d3fafe94"}},"02480d244ad54d3aa6c62fe196d4390c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e6ab287a2d964655a42b416c721e3e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89cf87c5673a4edebb8d2777f7d43c04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5db3ddda9c534222b41c2b98d3fafe94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1c8823c6df284687a0665faf753df024":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_18f8c655aa7247da927aa932fe845bb5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_22ba7559f54b40368a2788f401a2a048","IPY_MODEL_f34f0bddb547410fa1c9294fb0f27fa1"]}},"18f8c655aa7247da927aa932fe845bb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"22ba7559f54b40368a2788f401a2a048":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c88fadc8322842128ce8478a54990b65","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":443133604,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":443133604,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_669f0de609864bb0bff796a2ebf4ca7d"}},"f34f0bddb547410fa1c9294fb0f27fa1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a0bec739aec940eeaf722c259fa5fadd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 443M/443M [00:14&lt;00:00, 31.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6b367d928620499cb9350b6b0a852a9e"}},"c88fadc8322842128ce8478a54990b65":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"669f0de609864bb0bff796a2ebf4ca7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a0bec739aec940eeaf722c259fa5fadd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6b367d928620499cb9350b6b0a852a9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"cizlL4n8Bpf4"},"source":["#**준비 사항**"]},{"cell_type":"code","metadata":{"id":"ssi8SuijILoR"},"source":["from google.colab import drive\r\n","\r\n","drive.mount('/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H9_Mh0vo-vFy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608793330072,"user_tz":-540,"elapsed":9795,"user":{"displayName":"‍윤채훈[ 컴퓨터정보통신대학원석사과정재학 / 빅데이터융합학과 ]","photoUrl":"","userId":"04155508879667297547"}},"outputId":"f79bf0d8-ef30-48b0-ff3d-cd5b208b3884"},"source":["!pip install transformers\n","!pip install torch"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 12.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 51.1MB/s \n","\u001b[?25hCollecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 50.5MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=3e8c159669e3251bc7d3b5ebae87fb167ca4c6203e9ff62bc01dc9058959528b\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N_L_Zcoj_EUj","executionInfo":{"status":"ok","timestamp":1608793333505,"user_tz":-540,"elapsed":2445,"user":{"displayName":"‍윤채훈[ 컴퓨터정보통신대학원석사과정재학 / 빅데이터융합학과 ]","photoUrl":"","userId":"04155508879667297547"}}},"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import ElectraTokenizer\n","from transformers import ElectraForSequenceClassification, AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u-NlVvytBzCM"},"source":["#**데이터 로드**"]},{"cell_type":"code","metadata":{"id":"5IuLmQPvBXaQ","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"fc54dabc-6d0d-40a4-e696-c63aaffbbcd7"},"source":["!git clone https://github.com/e9t/nsmc.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'nsmc'...\n","remote: Enumerating objects: 14763, done.\u001b[K\n","remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n","Receiving objects: 100% (14763/14763), 56.19 MiB | 15.98 MiB/s, done.\n","Resolving deltas: 100% (1749/1749), done.\n","Checking out files: 100% (14737/14737), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jqIm07o_BgY8"},"source":["# train/test set 데이터 로드\n","\n","train = pd.read_csv(\"nsmc/ratings.txt\", sep='\\t')\n","test = pd.read_csv(\"ko_data.txt\", sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-iy1GdBgCHeJ"},"source":["label이 0이면 부정, label이 1이면 긍정"]},{"cell_type":"markdown","metadata":{"id":"AOELKVsKCPi6"},"source":["#**전처리**"]},{"cell_type":"code","metadata":{"id":"kcixMGgMtxev"},"source":["MAX_LEN = 128\n","\n","def getInputs(dataset):\n","  data = dataset.copy(deep=True)\n","\n","  if 'document' in data.columns:\n","    sentences = data['document']\n","  else:\n","    sentences = data['Sentence']\n","\n","  sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","  \n","  tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v2-discriminator\", do_lower_case=False)\n","  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","  attention_masks = []\n","  for seq in input_ids:\n","      seq_mask = [float(i>0) for i in seq]\n","      attention_masks.append(seq_mask)\n","\n","  return input_ids, attention_masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JYTqkJFO4d1"},"source":["def getIndex(dataset):\n","  data = dataset.copy(deep = True)\n","  input_index = data.index.tolist()\n","  return torch.tensor(input_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uH2SQ73eCmO_"},"source":["labels = train['label'].values\n","ratings_inputs, ratings_masks = getInputs(train)\n","test_inputs, test_masks = getInputs(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AMS6oW0-D7dm"},"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(ratings_inputs, labels, random_state=2018, test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(ratings_masks, ratings_inputs, random_state=2018, test_size=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zT3KyiOWN7cR"},"source":["# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\t\t\t\n","\n","test_index = getIndex(test)\n","test_inputs = torch.tensor(test_inputs)\n","test_masks = torch.tensor(test_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fkbv03zEN11"},"source":["batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","test_data = TensorDataset(test_index, test_inputs, test_masks)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SsLqFjfRF1FO"},"source":["#**모델 생성**"]},{"cell_type":"code","metadata":{"id":"aGQmXx4PFuwN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608793343605,"user_tz":-540,"elapsed":876,"user":{"displayName":"‍윤채훈[ 컴퓨터정보통신대학원석사과정재학 / 빅데이터융합학과 ]","photoUrl":"","userId":"04155508879667297547"}},"outputId":"f42c2d1a-3a00-4384-85c5-5028d031dc7e"},"source":["if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RwA7X59IeWFN","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b15d6033799c466cb747e759b8474fab","891594c5ce464b84ac6b02819cad096b","2ec4590d915142af9acd08cfd2de81dd","f99f572b85cf405987aff838afcb2be7","02480d244ad54d3aa6c62fe196d4390c","e6ab287a2d964655a42b416c721e3e4f","89cf87c5673a4edebb8d2777f7d43c04","5db3ddda9c534222b41c2b98d3fafe94","1c8823c6df284687a0665faf753df024","18f8c655aa7247da927aa932fe845bb5","22ba7559f54b40368a2788f401a2a048","f34f0bddb547410fa1c9294fb0f27fa1","c88fadc8322842128ce8478a54990b65","669f0de609864bb0bff796a2ebf4ca7d","a0bec739aec940eeaf722c259fa5fadd","6b367d928620499cb9350b6b0a852a9e"]},"outputId":"52744e67-4ce3-4692-cfb9-0185b3a96e82"},"source":["# ELECTRA 모델 생성\n","\n","model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v2-discriminator\", num_labels = 2)\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b15d6033799c466cb747e759b8474fab","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=487.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c8823c6df284687a0665faf753df024","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=443133604.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["ElectraForSequenceClassification(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(32200, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): ElectraClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"beWJCQyWIbZL"},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 3e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 4\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 학습률을 조금씩 감소시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cinFGiX2nkf2"},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jQAqEWO1nol6"},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfgQimfVnqBf","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"45521b00-bd58-44f9-c6dc-e9bb2d325608"},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로직과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch   500  of  5,625.    Elapsed: 0:10:27.\n","  Batch 1,000  of  5,625.    Elapsed: 0:20:53.\n","  Batch 1,500  of  5,625.    Elapsed: 0:31:20.\n","  Batch 2,000  of  5,625.    Elapsed: 0:41:47.\n","  Batch 2,500  of  5,625.    Elapsed: 0:52:15.\n","  Batch 3,000  of  5,625.    Elapsed: 1:02:45.\n","  Batch 3,500  of  5,625.    Elapsed: 1:13:15.\n","  Batch 4,000  of  5,625.    Elapsed: 1:23:46.\n","  Batch 4,500  of  5,625.    Elapsed: 1:34:17.\n","  Batch 5,000  of  5,625.    Elapsed: 1:44:47.\n","  Batch 5,500  of  5,625.    Elapsed: 1:55:17.\n","\n","  Average training loss: 0.29\n","  Training epcoh took: 1:57:55\n","\n","Running Validation...\n","  Accuracy: 0.87\n","  Validation took: 0:04:43\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch   500  of  5,625.    Elapsed: 0:10:30.\n","  Batch 1,000  of  5,625.    Elapsed: 0:21:00.\n","  Batch 1,500  of  5,625.    Elapsed: 0:31:31.\n","  Batch 2,000  of  5,625.    Elapsed: 0:41:59.\n","  Batch 2,500  of  5,625.    Elapsed: 0:52:28.\n","  Batch 3,000  of  5,625.    Elapsed: 1:02:56.\n","  Batch 3,500  of  5,625.    Elapsed: 1:13:25.\n","  Batch 4,000  of  5,625.    Elapsed: 1:23:53.\n","  Batch 4,500  of  5,625.    Elapsed: 1:34:19.\n","  Batch 5,000  of  5,625.    Elapsed: 1:44:47.\n","  Batch 5,500  of  5,625.    Elapsed: 1:55:15.\n","\n","  Average training loss: 0.21\n","  Training epcoh took: 1:57:52\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:04:43\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch   500  of  5,625.    Elapsed: 0:10:28.\n","  Batch 1,000  of  5,625.    Elapsed: 0:20:56.\n","  Batch 1,500  of  5,625.    Elapsed: 0:31:24.\n","  Batch 2,000  of  5,625.    Elapsed: 0:41:52.\n","  Batch 2,500  of  5,625.    Elapsed: 0:52:20.\n","  Batch 3,000  of  5,625.    Elapsed: 1:02:48.\n","  Batch 3,500  of  5,625.    Elapsed: 1:13:16.\n","  Batch 4,000  of  5,625.    Elapsed: 1:23:44.\n","  Batch 4,500  of  5,625.    Elapsed: 1:34:12.\n","  Batch 5,000  of  5,625.    Elapsed: 1:44:40.\n","  Batch 5,500  of  5,625.    Elapsed: 1:55:08.\n","\n","  Average training loss: 0.15\n","  Training epcoh took: 1:57:46\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:04:43\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch   500  of  5,625.    Elapsed: 0:10:28.\n","  Batch 1,000  of  5,625.    Elapsed: 0:20:57.\n","  Batch 1,500  of  5,625.    Elapsed: 0:31:26.\n","  Batch 2,000  of  5,625.    Elapsed: 0:41:55.\n","  Batch 2,500  of  5,625.    Elapsed: 0:52:22.\n","  Batch 3,000  of  5,625.    Elapsed: 1:02:50.\n","  Batch 3,500  of  5,625.    Elapsed: 1:13:19.\n","  Batch 4,000  of  5,625.    Elapsed: 1:23:46.\n","  Batch 4,500  of  5,625.    Elapsed: 1:34:14.\n","  Batch 5,000  of  5,625.    Elapsed: 1:44:41.\n","  Batch 5,500  of  5,625.    Elapsed: 1:55:07.\n","\n","  Average training loss: 0.11\n","  Training epcoh took: 1:57:44\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:04:43\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mFQWyso3ohCV"},"source":["#**test set 평가**"]},{"cell_type":"code","metadata":{"id":"7XW36KQToj4T","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"58addc47-17b1-4f58-9384-b17caba536c4"},"source":["tmp_test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=1)\n","test_result = test.copy(deep = True)\n","test_result['Predicted'] = 'default'\n","classes = [0, 1]\n","\n","#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(tmp_test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_index, b_input_ids, b_input_mask = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    idx = b_index.item()\n","    test_result['Predicted'][idx] = classes[np.argmax(logits)]\n","    \n","\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["  Batch   100  of    350.    Elapsed: 0:00:03.\n","  Batch   200  of    350.    Elapsed: 0:00:05.\n","  Batch   300  of    350.    Elapsed: 0:00:08.\n","  Batch   400  of    350.    Elapsed: 0:00:10.\n","  Batch   500  of    350.    Elapsed: 0:00:13.\n","  Batch   600  of    350.    Elapsed: 0:00:16.\n","  Batch   700  of    350.    Elapsed: 0:00:18.\n","  Batch   800  of    350.    Elapsed: 0:00:21.\n","  Batch   900  of    350.    Elapsed: 0:00:23.\n","  Batch 1,000  of    350.    Elapsed: 0:00:26.\n","  Batch 1,100  of    350.    Elapsed: 0:00:29.\n","  Batch 1,200  of    350.    Elapsed: 0:00:31.\n","  Batch 1,300  of    350.    Elapsed: 0:00:34.\n","  Batch 1,400  of    350.    Elapsed: 0:00:36.\n","  Batch 1,500  of    350.    Elapsed: 0:00:39.\n","  Batch 1,600  of    350.    Elapsed: 0:00:42.\n","  Batch 1,700  of    350.    Elapsed: 0:00:44.\n","  Batch 1,800  of    350.    Elapsed: 0:00:47.\n","  Batch 1,900  of    350.    Elapsed: 0:00:49.\n","  Batch 2,000  of    350.    Elapsed: 0:00:52.\n","  Batch 2,100  of    350.    Elapsed: 0:00:55.\n","  Batch 2,200  of    350.    Elapsed: 0:00:57.\n","  Batch 2,300  of    350.    Elapsed: 0:01:00.\n","  Batch 2,400  of    350.    Elapsed: 0:01:02.\n","  Batch 2,500  of    350.    Elapsed: 0:01:05.\n","  Batch 2,600  of    350.    Elapsed: 0:01:08.\n","  Batch 2,700  of    350.    Elapsed: 0:01:10.\n","  Batch 2,800  of    350.    Elapsed: 0:01:13.\n","  Batch 2,900  of    350.    Elapsed: 0:01:15.\n","  Batch 3,000  of    350.    Elapsed: 0:01:18.\n","  Batch 3,100  of    350.    Elapsed: 0:01:20.\n","  Batch 3,200  of    350.    Elapsed: 0:01:23.\n","  Batch 3,300  of    350.    Elapsed: 0:01:26.\n","  Batch 3,400  of    350.    Elapsed: 0:01:28.\n","  Batch 3,500  of    350.    Elapsed: 0:01:31.\n","  Batch 3,600  of    350.    Elapsed: 0:01:33.\n","  Batch 3,700  of    350.    Elapsed: 0:01:36.\n","  Batch 3,800  of    350.    Elapsed: 0:01:39.\n","  Batch 3,900  of    350.    Elapsed: 0:01:41.\n","  Batch 4,000  of    350.    Elapsed: 0:01:44.\n","  Batch 4,100  of    350.    Elapsed: 0:01:46.\n","  Batch 4,200  of    350.    Elapsed: 0:01:49.\n","  Batch 4,300  of    350.    Elapsed: 0:01:52.\n","  Batch 4,400  of    350.    Elapsed: 0:01:54.\n","  Batch 4,500  of    350.    Elapsed: 0:01:57.\n","  Batch 4,600  of    350.    Elapsed: 0:01:59.\n","  Batch 4,700  of    350.    Elapsed: 0:02:02.\n","  Batch 4,800  of    350.    Elapsed: 0:02:05.\n","  Batch 4,900  of    350.    Elapsed: 0:02:07.\n","  Batch 5,000  of    350.    Elapsed: 0:02:10.\n","  Batch 5,100  of    350.    Elapsed: 0:02:12.\n","  Batch 5,200  of    350.    Elapsed: 0:02:15.\n","  Batch 5,300  of    350.    Elapsed: 0:02:18.\n","  Batch 5,400  of    350.    Elapsed: 0:02:20.\n","  Batch 5,500  of    350.    Elapsed: 0:02:23.\n","  Batch 5,600  of    350.    Elapsed: 0:02:25.\n","  Batch 5,700  of    350.    Elapsed: 0:02:28.\n","  Batch 5,800  of    350.    Elapsed: 0:02:30.\n","  Batch 5,900  of    350.    Elapsed: 0:02:33.\n","  Batch 6,000  of    350.    Elapsed: 0:02:36.\n","  Batch 6,100  of    350.    Elapsed: 0:02:38.\n","  Batch 6,200  of    350.    Elapsed: 0:02:41.\n","  Batch 6,300  of    350.    Elapsed: 0:02:43.\n","  Batch 6,400  of    350.    Elapsed: 0:02:46.\n","  Batch 6,500  of    350.    Elapsed: 0:02:49.\n","  Batch 6,600  of    350.    Elapsed: 0:02:51.\n","  Batch 6,700  of    350.    Elapsed: 0:02:54.\n","  Batch 6,800  of    350.    Elapsed: 0:02:56.\n","  Batch 6,900  of    350.    Elapsed: 0:02:59.\n","  Batch 7,000  of    350.    Elapsed: 0:03:01.\n","  Batch 7,100  of    350.    Elapsed: 0:03:04.\n","  Batch 7,200  of    350.    Elapsed: 0:03:07.\n","  Batch 7,300  of    350.    Elapsed: 0:03:09.\n","  Batch 7,400  of    350.    Elapsed: 0:03:12.\n","  Batch 7,500  of    350.    Elapsed: 0:03:14.\n","  Batch 7,600  of    350.    Elapsed: 0:03:17.\n","  Batch 7,700  of    350.    Elapsed: 0:03:20.\n","  Batch 7,800  of    350.    Elapsed: 0:03:22.\n","  Batch 7,900  of    350.    Elapsed: 0:03:25.\n","  Batch 8,000  of    350.    Elapsed: 0:03:27.\n","  Batch 8,100  of    350.    Elapsed: 0:03:30.\n","  Batch 8,200  of    350.    Elapsed: 0:03:32.\n","  Batch 8,300  of    350.    Elapsed: 0:03:35.\n","  Batch 8,400  of    350.    Elapsed: 0:03:38.\n","  Batch 8,500  of    350.    Elapsed: 0:03:40.\n","  Batch 8,600  of    350.    Elapsed: 0:03:43.\n","  Batch 8,700  of    350.    Elapsed: 0:03:45.\n","  Batch 8,800  of    350.    Elapsed: 0:03:48.\n","  Batch 8,900  of    350.    Elapsed: 0:03:51.\n","  Batch 9,000  of    350.    Elapsed: 0:03:53.\n","  Batch 9,100  of    350.    Elapsed: 0:03:56.\n","  Batch 9,200  of    350.    Elapsed: 0:03:58.\n","  Batch 9,300  of    350.    Elapsed: 0:04:01.\n","  Batch 9,400  of    350.    Elapsed: 0:04:04.\n","  Batch 9,500  of    350.    Elapsed: 0:04:06.\n","  Batch 9,600  of    350.    Elapsed: 0:04:09.\n","  Batch 9,700  of    350.    Elapsed: 0:04:11.\n","  Batch 9,800  of    350.    Elapsed: 0:04:14.\n","  Batch 9,900  of    350.    Elapsed: 0:04:16.\n","  Batch 10,000  of    350.    Elapsed: 0:04:19.\n","  Batch 10,100  of    350.    Elapsed: 0:04:22.\n","  Batch 10,200  of    350.    Elapsed: 0:04:24.\n","  Batch 10,300  of    350.    Elapsed: 0:04:27.\n","  Batch 10,400  of    350.    Elapsed: 0:04:29.\n","  Batch 10,500  of    350.    Elapsed: 0:04:32.\n","  Batch 10,600  of    350.    Elapsed: 0:04:35.\n","  Batch 10,700  of    350.    Elapsed: 0:04:37.\n","  Batch 10,800  of    350.    Elapsed: 0:04:40.\n","  Batch 10,900  of    350.    Elapsed: 0:04:42.\n","  Batch 11,000  of    350.    Elapsed: 0:04:45.\n","  Batch 11,100  of    350.    Elapsed: 0:04:48.\n","\n","Test took: 0:04:50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7AXvRLrp88Uo"},"source":["test_csv = test_result.to_csv('test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bANOBRz-ft7f","colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"9085d4f3-9fe2-4fe6-fd1d-e8bcb3aa7bee"},"source":["from google.colab import files\n","\n","files.download('test.csv')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_f24d6e67-53a1-4c23-ad97-010c327292e0\", \"test.csv\", 913709)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"d-2e3pUGlCfD"},"source":["**코드 참고: https://blog.naver.com/horajjan/221739630055**"]},{"cell_type":"code","metadata":{"id":"dFdP0O4qlL3t"},"source":[""],"execution_count":null,"outputs":[]}]}